{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import *\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, ReLU, MaxPool2D,InputLayer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU   \n",
    "from keras import optimizers, regularizers\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import imgaug.augmenters as iaa\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import sys\n",
    "# sys.stdout = open('./code/adapted_deep_embeddings/log.txt','wt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_num_each_class(train_y, test_y, val_y):\n",
    "    zero, one, two, three, four = 0, 0, 0, 0, 0\n",
    "    for e in train_y:\n",
    "        if e == 0:\n",
    "            zero += 1\n",
    "        elif e == 1:\n",
    "            one += 1\n",
    "        elif e == 2:\n",
    "            two += 1\n",
    "        elif e == 3:\n",
    "            three += 1\n",
    "        elif e == 4:\n",
    "            four += 1\n",
    "\n",
    "    print(\"each classes has # images in train:\\n\")\n",
    "    print(zero, one, two, three, four)\n",
    "\n",
    "    zero, one, two, three, four = 0, 0, 0, 0, 0\n",
    "    for e in test_y:\n",
    "        if e == 0:\n",
    "            zero += 1\n",
    "        elif e == 1:\n",
    "            one += 1\n",
    "        elif e == 2:\n",
    "            two += 1\n",
    "        elif e == 3:\n",
    "            three += 1\n",
    "        elif e == 4:\n",
    "            four += 1\n",
    "\n",
    "    print(\"each classes has # images in test:\\n\")\n",
    "    print(zero, one, two, three, four)\n",
    "\n",
    "    zero, one, two, three, four = 0, 0, 0, 0, 0\n",
    "    for e in val_y:\n",
    "        if e == 0:\n",
    "            zero += 1\n",
    "        elif e == 1:\n",
    "            one += 1\n",
    "        elif e == 2:\n",
    "            two += 1\n",
    "        elif e == 3:\n",
    "            three += 1\n",
    "        elif e == 4:\n",
    "            four += 1\n",
    "\n",
    "    print(\"each classes has # images in val:\\n\")\n",
    "    print(zero, one, two, three, four)\n",
    "\n",
    "'''\n",
    "CBR model\n",
    "'''\n",
    "# def get_model(input_shape):\n",
    "#   kernel_size = 7\n",
    "#   model = Sequential([\n",
    "#     InputLayer(input_shape=input_shape),\n",
    "#     Conv2D(32,kernel_size ),\n",
    "#     BatchNormalization(),\n",
    "#     ReLU(),\n",
    "#     MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "#     Conv2D(64,kernel_size , input_shape=input_shape),\n",
    "#     BatchNormalization(),\n",
    "#     ReLU(),\n",
    "#     MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "#      Conv2D(128,kernel_size , input_shape=input_shape),\n",
    "#     BatchNormalization(),\n",
    "#     ReLU(),\n",
    "#     MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "#      Conv2D(256,kernel_size , input_shape=input_shape),\n",
    "#     BatchNormalization(),\n",
    "#     ReLU(),\n",
    "#     MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "#      Conv2D(512,kernel_size , input_shape=input_shape),\n",
    "#     BatchNormalization(),\n",
    "#     ReLU(),\n",
    "#     GlobalAveragePooling2D(),\n",
    "#     Dense(5, activation='sigmoid'),\n",
    "#   ])\n",
    "#   return model\n",
    "\n",
    "\n",
    "# '''\n",
    "# resnet50\n",
    "# '''\n",
    "# def get_model(input_shape):\n",
    "  \n",
    "#   base_model =ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "#   #for layer in  base_model.layers[:10]:\n",
    "#     #layer.trainable = False\n",
    "#     #layer.padding='same'\n",
    " \n",
    "#   #for layer in  base_model.layers[10:]:\n",
    "#     #layer.trainable = True\n",
    "#     #layer.padding='same'\n",
    "    \n",
    "# #   x = base_model.get_layer('avg_pool').output\n",
    "#   x = base_model.output\n",
    "#   x = GlobalAveragePooling2D()(x)\n",
    "#   # x = BatchNormalization()(x)\n",
    "# #   x = Dropout(0.5)(x)\n",
    "\n",
    "# #   x = Flatten() (x)\n",
    "# #   x = Dropout(0.5)(x)\n",
    "#   # x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "# #   # x = BatchNormalization()(x)\n",
    "# #   x = Dropout(0.5)(x)\n",
    "# #   x = Dense(32, activation='relu')(x)\n",
    "#   # x = Dense(128, activation='relu')(x)\n",
    "#   # x = Dropout(0.5)(x)\n",
    "#   x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "# #   x = Dense(512, activation='relu')(x)\n",
    "#   # x = LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "#   x = Dropout(0.3)(x)\n",
    "#   #x = Dense(5, activation='softmax')(x)\n",
    "#   #model = Model(base_model.input, x)\n",
    "#   predictions = Dense(5, activation='softmax')(x)\n",
    "#   model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# #   for layer in model.layers[:-2]:\n",
    "# #     layer.trainable = False\n",
    "\n",
    "#   return model\n",
    "\n",
    "'''\n",
    "VGG16\n",
    "'''\n",
    "# def get_model(input_shape):\n",
    "  \n",
    "#   image_input = Input(shape = (224,224, 3))\n",
    "#   model = VGG16(input_tensor = image_input, weights = 'imagenet')\n",
    "#   # model = ResNet50(weights='imagenet', include_top=False, input_tensor = image_input)\n",
    "#   # model.summary()\n",
    "#   last_layer = model.get_layer('block5_pool').output\n",
    "#   x = Flatten(name='flatten')(last_layer)\n",
    "#   x = Dense(128, activation='relu', name='fc1')(x)\n",
    "#   x = Dense(128, activation='relu', name='fc2')(x)\n",
    "#   out = Dense(5, activation='softmax', name='output')(x)\n",
    "#   custom_vgg_model2 = Model(image_input, out)\n",
    "#   for layer in custom_vgg_model2.layers[:-3]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "#   return custom_vgg_model2\n",
    "\n",
    "def split_data(data_dict):\n",
    "    trainset = []\n",
    "    valset = []\n",
    "    testset=[]\n",
    "    for label, images in data_dict.items():\n",
    "        random.shuffle(images) #shuffle each class\n",
    "        img_train, img_test = train_test_split(images, test_size=0.2)\n",
    "        img_train, img_val = train_test_split(img_train,test_size=0.2)\n",
    "        trainset = trainset + img_train\n",
    "        valset = valset + img_val\n",
    "        testset = testset + img_test\n",
    "    \n",
    "    #three dataset are stored in order data[class0, class1 ... class4]\n",
    "    #will do futher shuffle \n",
    "    return trainset, valset, testset\n",
    "\n",
    "'''\n",
    "output dataset like\n",
    "data = {\n",
    "        0:[[class_0_nparray, label_0]],\n",
    "        1:[[class_1_nparray, label_1]],\n",
    "        2:[[class_2_nparray, label_2]],\n",
    "        3:[[class_3_nparray, label_3]],\n",
    "        4:[[class_4_nparray, label_4]]\n",
    "    }\n",
    "'''\n",
    "def create_data(images_dict):\n",
    "    data = {\n",
    "        0:[],\n",
    "        1:[],\n",
    "        2:[],\n",
    "        3:[],\n",
    "        4:[]\n",
    "    }\n",
    "    for label, img_paths in images_dict.items():\n",
    "        for img_path in img_paths:\n",
    "            img = image.load_img(img_path, target_size=(224,224))\n",
    "            img = image.img_to_array(img)\n",
    "#             features = preprocess_input(img,  mode='tf', data_format='channels_last')\n",
    "#             img = img.astype('float32') / 255.0\n",
    "            data[label].append([img, label])\n",
    "\n",
    "    return data\n",
    "\n",
    "'''\n",
    "custom data augmentation\n",
    "'''\n",
    "def create_custom_gen(img_gen):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.MultiplyHue((0.5, 1.5)),\n",
    "        iaa.imgcorruptlike.Contrast(severity=1)\n",
    "    ])\n",
    "    for X_batch, y_batch in img_gen:\n",
    "        hue = seq(images = X_batch.astype(np.uint8))\n",
    "        yield hue.astype('float32')/255.0, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_5classes_SparseCross_sigmoid_100_epoch80_imagenet\n",
      "0 images: 100, four images: 100\n",
      "1 images: 100, two images: 100, three images: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_base = 'E:\\\\aptos\\\\labelsbase15.json'\n",
    "classes = n = 5  #TODO:\n",
    "\n",
    "k= 100 #TODO\n",
    "# path_base = \"/home/z5163479/code/base15.json\"\n",
    "# path_novel = \"/home/z5163479/code/novel15.json\"\n",
    "with open(path_base, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "labels = np.array(data['image_labels'])\n",
    "images = np.array(data['image_names'])\n",
    "\n",
    "epoch = 80\n",
    "NN_layer = \"resnet_{}classes_SparseCross_sigmoid_{}_epoch{}_imagenet\".format(classes,k,epoch) #TODO\n",
    "BS = 16 #batch size\n",
    "print(NN_layer)\n",
    "\n",
    "zero_images = images[labels == 0][:k]\n",
    "one_images = images[labels == 1][:k]\n",
    "two_images = images[labels == 2][:k]\n",
    "three_images = images[labels == 3][:k]\n",
    "four_images1 = images[labels == 4][:k]\n",
    "\n",
    "# add more data for class 4\n",
    "four_images2 = []\n",
    "\n",
    "if len(four_images1) < k :\n",
    "    path_base = 'E:\\\\aptos\\\\labelsnovel15.json'\n",
    "    print(\"adding image from second dataset\\n\")\n",
    "#     path_novel = \"/home/z5163479/code/novel15.json\"\n",
    "    with open(path_base, 'r') as f:\n",
    "        add_data = json.load(f)\n",
    "    add_labels = np.array(add_data['image_labels'])\n",
    "    add_images = np.array(add_data['image_names'])\n",
    "    n = k - len(four_images1)\n",
    "    four_images2 = add_images[add_labels == 4][:n]\n",
    "\n",
    "four_images = [y for x in [four_images1, four_images2] for y in x]\n",
    "\n",
    "print(\"0 images: {}, four images: {}\".format(len(zero_images), len(four_images)))\n",
    "print(\"1 images: {}, two images: {}, three images: {}\".format(len(one_images), len(two_images), len(three_images)))\n",
    "trainset = {\n",
    "    0:zero_images,\n",
    "    1:one_images,\n",
    "    2:two_images,\n",
    "    3:three_images,\n",
    "    4:four_images\n",
    "}\n",
    "\n",
    "path_base = 'E:\\\\aptos\\\\public15.json'\n",
    "# path_base = 'E:\\\\aptos\\\\private15.json'\n",
    "with open(path_base, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "test_labels = np.array(data['image_labels'])\n",
    "test_images = np.array(data['image_names'])\n",
    "\n",
    "c = 10\n",
    "zero = test_images[test_labels == 0][:c]\n",
    "one = test_images[test_labels == 1][:c]\n",
    "two = test_images[test_labels == 2][:c]\n",
    "three = test_images[test_labels == 3][:c]\n",
    "four = test_images[test_labels == 4][:c]\n",
    "# print(\"0 images: {}, four images: {}\".format(len(zero), len(four)))\n",
    "# print(\"1 images: {}, two images: {}, three images: {}\".format(len(one), len(two), len(three)))\n",
    "\n",
    "testset = {\n",
    "    0:zero,\n",
    "    1:one,\n",
    "    2:two,\n",
    "    3:three,\n",
    "    4:four\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(images_dict):\n",
    "  dataset = []\n",
    "  for label, img_paths in images_dict.items():\n",
    "    for img_path in img_paths:\n",
    "        img = image.load_img(img_path, target_size=(224,224))\n",
    "        img = image.img_to_array(img)\n",
    "#             features = preprocess_input(img,  mode='tf', data_format='channels_last')\n",
    "#             img = img.astype('float32') / 255.0\n",
    "        dataset.append([img, label])\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_train = get_data(trainset)\n",
    "img_test = get_data(testset)\n",
    "# img_train, val_test, img_test = split_data(data)\n",
    "\n",
    "# print(len(val_test))\n",
    "print(len(img_train))\n",
    "print(len(img_test))\n",
    "# val_size = len(val_test)\n",
    "train_size = len(img_train)\n",
    "test_size = len(img_test)\n",
    "# assert val_size + train_size + test_size == k * classes\n",
    "\n",
    "'''\n",
    "Split three datasets into X and Y two arrays\n",
    "    - shuffle each dataset, then split\n",
    "    - use resnet50 data preprocess method to preprocess X\n",
    "'''\n",
    "# val_x = []\n",
    "# val_y = []\n",
    "# random.shuffle(val_test) \n",
    "# for features, label in val_test:\n",
    "# #     features = preprocess_input(features,  mode='torch', data_format='channels_last')\n",
    "#     val_x.append(features)\n",
    "#     val_y.append(label)\n",
    "\n",
    "# val_x=np.array(val_x).reshape(val_size,224,224,3)\n",
    "# val_x = val_x.astype('float32') / 255.0\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "random.shuffle(img_train)\n",
    "for features, label in img_train:\n",
    "#     features = preprocess_input(features)\n",
    "    train_x.append(features)\n",
    "    train_y.append(label)\n",
    "\n",
    "train_x=np.array(train_x).reshape(train_size,224,224,3)\n",
    "# train_x = train_x.astype('float32') / 255.0\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "random.shuffle(img_test)\n",
    "for features, label in img_test:\n",
    "#     features = preprocess_input(features)\n",
    "    test_x.append(features)\n",
    "    test_y.append(label)\n",
    "\n",
    "test_x=np.array(test_x).reshape(test_size,224,224,3)\n",
    "# test_x = test_x.astype('float32')/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] [10 10 10 10 10]\n",
      "[0 1 2 3 4] [100 100 100 100 100]\n"
     ]
    }
   ],
   "source": [
    "index, count = np.unique(test_y, return_counts=True)\n",
    "print(index, count)\n",
    "\n",
    "index, count = np.unique(train_y, return_counts=True)\n",
    "print(index, count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=to_categorical(train_y)\n",
    "test_y=to_categorical(test_y)\n",
    "# val_y=to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0 0.0 78.7741 59.578827\n",
      "255.0 0.0 87.871414 61.65179\n",
      "(50, 224, 224, 3)\n",
      "(500, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.max(), train_x.min(), train_x.mean(), train_x.std())\n",
    "# print(val_x.max(), val_x.min(), val_x.mean(), val_x.std())\n",
    "print(test_x.max(), test_x.min(), test_x.mean(), test_x.std())\n",
    "\n",
    "print(test_x.shape)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\clc87\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\clc87\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clc87\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "resnet50\n",
    "'''\n",
    "def get_model(input_shape):\n",
    "  \n",
    "  base_model =ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  #for layer in  base_model.layers[:10]:\n",
    "    #layer.trainable = False\n",
    "    #layer.padding='same'\n",
    "  #for layer in  base_model.layers[10:]:\n",
    "    #layer.trainable = True\n",
    "    #layer.padding='same'\n",
    "    \n",
    "#   x = base_model.get_layer('avg_pool').output\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  # x = BatchNormalization()(x)\n",
    "#   x = Dropout(0.5)(x)\n",
    "\n",
    "#   x = Flatten() (x)\n",
    "#   x = Dropout(0.5)(x)\n",
    "  # x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Dropout(0.5)(x)\n",
    "#   x = Dense(32, activation='relu')(x)\n",
    "  x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "#   x = Dense(512, activation='relu')(x)\n",
    "  # x = LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "#   x = Dropout(0.3)(x)\n",
    "  #x = Dense(5, activation='softmax')(x)\n",
    "  #model = Model(base_model.input, x)\n",
    "  predictions = Dense(5, activation='sigmoid')(x)\n",
    "  model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#   for layer in model.layers[:-8]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "  return model\n",
    "\n",
    "model50 = get_model(input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\clc87\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfine-tune some conv layers\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 80\n",
    "BS = 16 #batch size\n",
    "\n",
    "'''\n",
    "Image augmentation\n",
    "'''\n",
    "image_gen = ImageDataGenerator(\n",
    "#                               rescale=1./255\n",
    "#                             rotation_range=45,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "#                             zoom_range=0.2,\n",
    "#                             shear_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "#                             fill_mode='nearest'\n",
    "                           data_format='channels_last'\n",
    "                           )\n",
    "\n",
    "img_gen=image_gen.flow(train_x, train_y, batch_size=BS, shuffle=True)\n",
    "# img_gen = create_custom_gen(img_gen)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "                                )\n",
    "\n",
    "# val_gen=test_datagen.flow(val_x, val_y, batch_size=16, shuffle=False)\n",
    "test_gen=test_datagen.flow(test_x, test_y, batch_size=20, shuffle=False)\n",
    "\n",
    "# check_num_each_class(train_y, test_y, val_y)\n",
    "\n",
    "# model50.summary()\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "# sgd2 = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
    "\n",
    "model50.compile(\n",
    "#                 optimizer=adam,\n",
    "                  optimizers.RMSprop(lr=2e-5),\n",
    "#                 optimizer=sgd2,\n",
    "#                     loss='categorical_crossentropy',\n",
    "                    # loss='kullback_leibler_divergence',\n",
    "                    loss= 'binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "# Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'cate'\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                      write_graph=True, write_images=False)\n",
    "\n",
    "'''\n",
    "fine-tune some conv layers\n",
    "'''\n",
    "# # # train_model=model50.fit(train_x, train_y, batch_size=8,epochs=epoch,verbose=1,validation_data=(val_x, val_y), callbacks=[tensorboard])\n",
    "# # train_model = model50.fit_generator(img_gen, validation_data=(val_x, val_y), epochs=10, steps_per_epoch=len(train_x)//BS, verbose=1)\n",
    "\n",
    "# # ## start train\n",
    "# # for layer in model50.layers[:165]:\n",
    "# #   layer.trainable = False\n",
    "# # for layer in model50.layers[165:]:\n",
    "# #   layer.trainable = True\n",
    "\n",
    "# model50.compile(optimizer=optimizers.Adam(lr=1e-5)  ,\n",
    "#                         # loss='binary_crossentropy',\n",
    "#                         loss='sparse_categorical_crossentropy',\n",
    "#                     # loss='kullback_leibler_divergence',\n",
    "#                     metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "255.0 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO29eay8WXnn9zlVdZff0htLA0MD3Y0bhyWhYQCT9Bic4LEBjdJ2FE9Akd1yrGBLRrKViRRsR4mVvyaTwZasJIywjIwjD7YzHo/RiGSMkBUyM8E2MJjF7TbNZnpx7/1b7lbbyR/n/f7eb5371r11b1Xdqrp1vlKp3nr3t95znvMs3+c5IcZIQUHB+qK16BsoKChYLIoQKChYcxQhUFCw5ihCoKBgzVGEQEHBmqMIgYKCNcfchEAI4d0hhIdCCA+HED40r+sUFBRMhzAPnkAIoQ38FfB3gUeAPwPeH2P8i5lfrKCgYCrMSxN4G/BwjPGbMcYu8DvA/XO6VkFBwRTozOm8Lwe+a78fAb5v3M4hhEJbnAQbF6E3qH5ECC2IfSCk3wRgWC3naAED2++s//KQ/Y5Am3S/bbufYPvGhuVWtaxPy47TflTr9ZxDCG2IETpt6O/M6qFWDU/HGF+cr5yXEMjfOGStLoTwAeADc7r++cSLXwuPXYPOAPp92LgA3WepO1EADoB9O6hD6vwXgOvVtg7QG3ORdrX/rKDzdag7cAvoA5eAbvXdr7ZtVJ9WdY9tagEWgIskwdGt1un3lp0jAtvQ3oBBD9iD7VtgbwC33QxP/dsZPt9K4TtNK+clBB4BXmG/7wAe8x1ijB8FPgpFEygoWCTm5RP4M+CeEMJdIYRN4H3AJ+d0rYKCgikwF00gxtgPIXwQ+Fckfe5jMcavzeNaBQUF02Fe5gAxxk8Bn5rX+dcS1w+APoTK4dV1pyCMOtIG1LY3JPtadvXwiIscte00yB177rgbMmrvy2fQZ/Q5gp0rkvwe8jMcANu2fVh9D2Cg8wfoV8ftjvOFrC/mJgQKZogX3Qt7e3B1h9RBNkidYI86GiBHmpx66sxdaudcj7rTjcOs3TN9W646541mN6B28m1QOzgPqmWPFEig7VM7N+UI3aD+D7RfqL630nV6lcN0Zxfar4dL23D1CzN+1tVEEQLLjNvfDDtdeP5KNZL1gAgDdeRh9d0ndZpu9ZGHnOpb+8DsR/pJoQ6r6+v++6SRXB13QD36uzYgwXfAaGfH1m1V+12vzqn/rE8SmKTvAXB1CHwvXLwJdj8/+8ddIRQhsIy4+x3w7BW4ugu9HrSl5u4BAYZSmfepO0OPevTLR/NFB190jzIJtNyn1kyk0UhzUbjQTQNpNNpvs/qWmbBv55HaH7N17eqclWDcbQHfU4UTH5zL0y87ihBYJrzibfDsdXjm6dT597tpfcttZan8LW4IBaC2r13dn2W8fxq49mEazQ0/wAGpKcaG/bQsk0Ho2THV/0SrWtb/Je2obffhAqhD0hqAwR7wt8gi2WuBIgSWAff8HXh2F57fSR2+N4ADjXA96LlaO6Bu9BpVBbHvlh0x+4baV+BCDWrm39DW6Rml/UiTkGawQT36y1SS4LwGXLbraz+Rkl5YfT819VOuCooQWCja8LI3wpPPQxzC/j4QoLdHapib1B1/jzRiysaH5pF+WUb/SaGOLXVenV7fZMvYdv03UJsNkdSstU3awpDDgmaX9B9LQ5BwOQBuB56czSMuOYoQWBRe9lbY7aYR/6ALw0HV+eXE0ygv9VaRgPOGJv+FRmd14AG1n0AageVQjPxfChe27DzaP9h5ZFb1bLuuKaFwU3WuazN61uVEEQJnjde8E555Bp65BmEfhiHlAUTZqgfVjl3S65FNex4FADQLAUv8GUkw0vLAjpPQ3LRzSGj0bFmfLqMdXv4JOSl3qv32qZ2It1TLz87kiZcNRQicGbbgxa+Bp56D7j4MBjDcg6gGvlvt517+yiewcO/+vDAuWUnhQanoLggUQtRv50gEavVeiUgSqhvU4VP5EfTtiU2uiWj5Cin8eIkkJM4XihA4C7zuB+GJK7D/LBwMoTWEVoCBGqEYcB7LV+d3h9h5wzj/hZ47X6eRXJ08MCow5ENxFqLsf88wVEQiZysqG/GgOpe4BS5kLlb7PHeqJ15GlBqD88ZL3g6PPA30oTeEbhe6YrANSQ1Oyz3qKAAN3+uCnFoMtTD0OgQ5ZM9Lg5IPYMP2l+CVqq8ojEwQCYY+o5wLF8hOVV59FE1gnrj9ndC9Dv3dpPbvVw1x2CXFpy8waqOummd/3lCHzKnQWu7YfqqbIA1B0YO2bfOoigSMBID8A04sUlRC2oKEgjSOzey8q4kiBOaBV90H3Qi7z8LuXlUN6Dqpw4vwA8nWXLdRflI4B0LsQaj9AyJHbVHzJqBmEqpzSjsQl6Bnx2/bse4DyCMLCjnKZ+DnoLpeh9qvs1ooQmCWeM33w5XrsN9LGWyDQSprRY9a7ZcQaLJ7C0ahUdpDfL7eY/teasyFh1R49wlI61IFJjcdnH0ozULrPCwp7UPbdkkhxX3GV21aThQhMCvc+U547ioc7AIb0D2o+CqB5GDqkhrKcVl86wap1s4Q9DqJOS1aEQUPI+oYZVjCqNDIeQU6t4RFx749ganD6LtyH4U0Ap1b97VJMvOuTvoHLByndgyGEF4RQvjjEMKDIYSvhRB+rlr/yyGER0MIX6o+753d7S4p7r4vqf2xD60N6FYjfX8P4nVqpp8aakGCuP5NGpHyIGTfe2qxVHSRibRN8X4XIDq3j3cayfPipO4H8GzHfB2MChbdr87VpaYmLz+m0QT6wD+IMX4xhHAT8IUQwqerbb8aY/zH09/eCuCOt8L+AHrdRP3tAcMeDESDVaNYLRXxbOCdykdtqfjOGGwx2iFdKATbT6QgH9G1r5ONdA3nCkDtpJVAye9TcFKThIEiO+IwKNy43Di1EIgxPg48Xi1fCyE8SCo1vj541Q/A80/B1sXU8fcHELspAzB2qYtjeNiv4DDUSZ244+XTfRTWb69EpP1UXARqX4CbDdj2nIosJ+JJTDXXTFwz0Lv2sufLi5nwBEIIdwJvAv6kWvXBEMKXQwgfCyHcNotrLBVe+hq4441w/Woq9bWzW7EA+9A/gOgFP2Q3FoyHCobkKnefUftcI7n2h9FEIgmFHQ4XXfGogqB98hoEk0JCBg53dL37SCIYLS+mFgIhhMvA7wM/H2O8CnwEeDVwL0lT+PCY4z4QQvh8CGG1yrq89j7gFugPYVDRULt7sNdLk1oM+9SNUFGBIgSOhswmTxXOR1B1cgkBkXlcSHiCUA7RiYWmqTFOAxcEDmks4h9cnOE1Z4up5iIMIWwA/xL4VzHGX2nYfifwL2OMbzjmPMutLwkvfTtsdmEnJgLQADjoV9l/UDsAr7PKcePlgISAd2zVDGhRpx2LvOPEH6ddy6OvWH/TNaaBxtFxgl7aR1XrEFigf+gLMca35CuniQ4E4DeAB10AhBBeZrv9KPDV015jqXDTHTA8gJ29NNrvD1O1354aozLWRAjaP+psBcfC6boa8ZVl6ck/gdrxqg+MOg2h2Sczi7EnN2GakCc7LRemiQ7cB/w48JUQwpeqdb8IvD+EcC/pib8N/PRUd7gMuPtNsNuGnZ1Uhiq0oN8iTXElNbVP3UCL+j85TjMad7PfygNwx577ETy+P2sc50iUFuJC4AJ1EZXFY5rowL+m2cg5X3MN3HI3XBumkF+/YpZ1D6p3L/qo+P9OCS6YDNOOxs4C3KI2A7x46aKh+3NzpUXiElxf4H0lFMbgUbjrh6D3TKoA1BMBiFQE5IYNKg2gqcpvwfzhqdbXGDUBTvI+Zj0RqyBfRpNvA5JWsNd86BmhCIGj0Hs+Ffnc30taQCCRgkYKW7gPoAiBxcNpweOQcxHykOOsTQd3TIrfAMtSMKYIgSbc8x/Bc3upw/erRKB+H4Yed9ZHzqjFv8yCSeDlx5us2Xn5DpSF6KXQlLjUFLk4OxQh0ITr+xB7yfs/iBBj1cc16osD4CWvCxYLje5edKRJMHvFIXV4nxhlnsI81wj0e8AiKcZFCOS44+0pAajVgt2DivqvNGCFAD2DrGBx8E7ry8cJ5fwY/VbewQHjBULTeqn4gzG/HV4kNTJaDWlefomjsXxBy0Xie/8e7F+Ffjep/91eogPfyBEXA3Bc5lvB2SJnFJ50FFfIzpOHms497poOL1/mFY/GHS8Tcpfap6T6BWeLogkI33Mf7O/C3i5sbsO1vcQJGCGreN57wfmAv8/T5A/ouHzZw4LiMUyCsw8xF00A4JVvhytVmC+0U0HQgUZ+jfpaLjjfcKFwku7hTkaFAKXu61yTjvIXaHZazgdFCLzyDTBoJ/V/fz+9v71ucgwCdRmqkgq8ftCovsHxXUV+APkMNHOUFztV3sNxwmDIWWYeFiHANjeYf1d3YL9fZQcqHKhPCQGuB/IKRFDXE3QiUg4vj+a1DX0ZDjsWvTaCQofafulUT3BSrLcQuOs+2L2eyEDDQUoFvuEDkLOm0IDXC7LjcwSO1wicqai0Z6iLyuwyOsmMjlGn73A4YjF/QbC+QuC1Pwi7+8BGKgne3Um+gBtCWqHAgvVCzjj0asNS9zeoR21pCa4BQO0Q1LLOjR3XZlRYCNI8xUOZL9ZUCGzA1etpboBWZaP1qxcd90hc7mVKQCk4O7hjUMVNFPOXo0+lzDTRiav+6rTilMDoSO/X2aL2FzhHwIXJ/J3R6ykEXv02oA29PuxV7MCeogESAsUMWG94URN1XrER1dFV5Ui/WzRH3TXi5+Fldfa+LXtWpLINL0z9NEdh/YTAPd8POz2IleOv20vzA6LCoD7TTMH6wqsSubqe1450ld6LhigCIE0iFwI+56GO7WfHerWkm2bzWA1YLyHwxnfB89dTUtCVK3DQS0VBI9RcAJUIKyjwkVoDg/j9brdH2+77trJz5Of2QiPuIPQ8CJ+ifj6YmjEYQvg2KZF7APRjjG8JIbwA+F3gTlJ1ob8fY1z8XM7PXak0rz0YtKragGIBKrY73z+8YJmRj9Y+gWnu3Nug9uar3XgZsTwCkOc5QN3OZA64IPFJWHsk/4ELodlhVprAfxxjvNeKGH4I+EyM8R7gM9XvxeJ77oPdPmy0oT+oGIHKCpQZUAqDrDeO8sTnUQPnjvgcCHkCkAuG/HwSDl76nGzfIXXlqg5w8wTPcTLMyxy4H/h4tfxx4EfmdJ2JsPnW96TCoJ1O6vydLRgoKxBG56kvQmC9kNN983AdNCvMQxKPRPNLNuUPKJKQ+wIkAJwc5NOYhey3UqPlj7jlmGc6GWYhBCLwRyGEL4QQPlCte0k1Q5FmKro9P+gs5x3oPnUFwqAyWIYpM3AIo+WiS4HQ9URuh0vFb9pnnKYgMpALgry4qOBmRZ5W7NtV8QhG51mYfc7fLM54X4zxsRDC7cCnQwh/OclBMcaPAh+FOc878KYfgmeuVCXCKtVs4LUAlDtetID1Rj4Bqav1TTn+7rzz6c78mLy+gVR/FRbJJziFUb6AprCD0SjCAHgR8PRET3YcptYEYoyPVd9PAn8AvA14QvMPVN9PTnudU+P5nfq/b0XoHSSfwKE5AsaVmypYL7htDuPbhJiDIvpATfwZB08tbpqvQAlHfl0JDAkHOQp3jnmOyTGVEAghXKpmJCaEcAn4IdJkI58EHqh2ewD4w2muc2q89l2wu1PNGjyEg4MqTXiXZMv5/PLFHCiAo1miss9hlE3o20Qe0rYOybMvgaGJapqukbc/kYXcsSjNoMusuAPTmgMvAf4gTUZEB/inMcb/O4TwZ8DvhRB+Cvhr4MemvM7Jcftr0xyBMaTU4P4QBkNoSdXaZ1S6FhTkHc7hiT3q6D1b79tFDtK8FB1bP8ncFE5UUtvUd6+6vkhK02OquQhnhZn7BC7eCbfcCu1tuPp8yhCMpIShqHJhyuhaTF23s8WtpIkuNqHdgcFzpEkvFlvvfrngo3RTfcDc0+8Cw4uGSgPokzQAmZzbtm+XWs1vMSokFBLMu4TPqQj1RCsnmvOycS7C81le7KV3wbWrMOzCMCYfwEG/Sg7y8I3MgPOMW6B9O1zYhBhT5aSdWyEqLHren39SaNQdVwrMHYDav4knoG0t+9bxOZtQJsUGo45B2f0hOyeMTuGuQewmEl/vdDh/tOF7fzip/u0WHOzDMCQTIB5QlwgblzN+3tABboGtzVQ6/aCXciW2tklJKedzDDgd1MnH/ScD2yd3HLqzL2TH+G9VM9b1OtQaiPbHzp+30Twi4YPY6UlE568VXL2WRv9hD/ox+QEC1H/WDuszV8BFoAUhpJoJN7Jd3aFVkJCPuEeFjLVNo7k6pCf++ES1rg0M7Vg/XpqD1xBw4lKeZ0C2fPqU4/PVEt7wzmrhII14gz7ELhxIYio2uw4CAG40sIOqgQz7iSl5oMhIKZx6GHkNgCa4GeDxfq8p4B1Y6n4+7VlOJhIHwMOOTiaS9qBryEHZJfkITpdyfL6EQK+dtIDuoLJ/Y5pKLKpsmLOw1gGVCdTfS7UTbrDOdkmOwcU7hVcbOdvQawN4yFBqP4wSi5wKrI7tAsC1BcGrFXn9y9MXJz1fQmAwSFyA/iAxBLv9JAxuOAGbEjzOM3ZJ5k+3+u6Rnn+HE3iUC0ag0Fzu0ffQYi5cnQDk0SitV9uUX8G1gUBdgWiDFNFxs0LOTOXCvPTET3S+fAKDXsUG7KcS4gP35kpqroND0HGVulGp0UgzKjgZ5PjzUV22f84vkL3ubU4qvOz+lu2rwWqYHdeUgrw95r7Efj0Zzo8QuPVO2N2D4TAJgqg/r0dNDFrXGYR2mM+U2+cZTY5B7+geFvQR3J12Du/cOhfUQiQ/Z7Btrmm4OSunItTa7skHufNjDrzojuQPGAxTAdGhmIAq0OD22jqiCICT4ah24rZ4HmkK9hk2fHLvfoe6aKkShDxaoP22bFn8AAkfr24USMSwyXF+NIH9avLQQUhe8LSS9Id57YCCgmkwLqMwzyT0mL9MCO/ckeTI0+gtX4E0AeW1bDKaSbjNqDPR/QKuGWwyafTnfGgCb313cgiGkJyBN7SA3GFTUDBreFkxV8WdMah2mCeqebhaHT93Lg7sPB07Rk5e9y2oYvaQw36D8TgfQuDp64kT0Oqk72FeJ66owgXTQqq2s/6EpkHGVX13KKrTd6hp23lpMTkJnUgUGa1ElIcaB9V2lcubvM2vvBAI3/de2O+m0mF7+4kpeEPqlklEC2aFo5iEfVvv8X+VENtmtOy45xlof7frfW4Dzx7ctXU6pm+/XePoArdN9GQrLwTiM7uwGaC9lYqGxGFlDkhdKr6AgllBHayp6IjMgZBtU/QAatveBYpSjV21z4uTOovRS+LnwkjbWpyEDbr6jsGDvVQ/cL9bzSIEoxVcS/nwglki73hyynkWoPbLIwcyEbp2rPILZGa478AFSNPkJbnZ69smT5E/tRAIIXwvaW4B4W7gfyAlr//XwFPV+l+MMX7qtNc5Em98Fzy7C7u7iRnY3We0osu6EYMKFgOvKyBIMKg9enlxb5saqLzTakRXhmGe3uxCpwkt6hmUj08zPrUQiDE+BNwLEEJoA4+Sagz+JPCrMcZ/fNpzT4znRaHcqP7XDqloiNdqKySZgnnDcwigucPrc8Cogw8Ohwmd5dqhZhiquzYVPWnCZE7xWfkE3gV8I8b4nRmd73i87j9MOfK9fkoZPtiD6DFWt4/OO8Y9Y9GEzgZqZ27Heyy/Z8t5uTAYTQbS+ZwyLHNB2+Dodq1QoRyWRzsIZ9VD3gd8wn5/MITw5RDCx0IIk7koT4oBQEhZgq2QcgVu/Fmqz+65A+cZXsFGyCfEOA5irBWcHFL5vdCH2qMIa2L2+XvKQ30+HZlU+pzz4rRiaO7Cfj8qbzYeUwuBEMIm8J8C/2e16iPAq0mmwuPAh8ccN93kI12ge1D5USTx9GcrxrpORCEfWZQsNClC9r0oqFTWC1hegTSuyzglvYn66/F/r0PoZCDnE/ixqhkAdRajlxnLsclosZOjTYdZaALvAb4YY3wCIMb4RIxxEGMcAr9OmofgEGKMH40xvqWp8OGxeNnrEzOwd5DKh/U1hZhULtXPG1cv7rzjpBrQsvAqbiL5lS+weIE0DrkDUOtgtMyYUn+3qBl+0lT9/TjZB0adieIAeLah2/ktav8CjHbnTUZLl/l+o5iFEHg/Zgpo0pEKP0qah2C2uHQbxAHs70O/l4qH7Ck9VvZQU8XWdcFpzKBl+K8GwLPVZ9ECaRw0oo9Tw73oh6vsXr9QyWzaxzu3Or4LlBaJcOQORbERHdIS3GSQhjG+BuFUPIEQwkXg7wI/bav/UQjh3uouvp1tmw2u7wHDVEw0DFIpMfap67mpyMK6RgXGqYlHYRmEgMrArwIUw8/j81B3xjb1RLe50HBnn6A6hdJqpTU4fdijBbqWTF8XAgeMMg/HC9WphECMcRd4Ybbux6c557F40Wsh9lPhzHY7+QP6A5LadY31mEfgOOTFLVZFGM77vR1HuDkJZEL5OXLCkEwC5Qg4I7Bv+/j7clteQkQmheYrULcdkDSELnXCkLQJL1QigdCM1Yuf3fbSpAEMh+nu9w6SaQCMsq0KElZFAMD8nYH5mDeL/2aYLSsq4JWBoDYh3J4X3HRzZ3Y+TZ47HLUsjcJHfJkCOjanM49i9WjDvR602kkIDAdVfxc1eFISRcFyYt7vzvn089KQ8sw+PZNsc6nqUtu9spD2kZbgGkLOMISaguzVjeQslKM3UM9W1IzV0wSG+6l60GCQvtvOt4bl8HIXnB6T58GfDj4yzwNuo++TOryr56LzakRXx9+049w5iB2v83u39XoFigh4ajJ2rWaslhC4529X0YB2mlMvRgiqvuIllwpWF1vH7zIV8uKd054r/y11XHwAdVDXEOT8c0eePP1OG3Z2oDsYXf1Xx/dBUD4IhVo9MnEYqyUEBluwO4But5pqvJuEwg1ewLrQhM8LmuzU08+pNxnUKWeB/Dxqe/ILaJ06Loyq/64NiEvQphaETRTjAaMOSZ27TR01cNNBfWK8qbVaPoG9fl1DcFCRgUKokoYUZy2z6qwOmkbkVXJkHgXvgE4A8gzAfraPZxfG7Fwwmj+gcKGTjDRLsYRI15bHC4HVGjYHu7DRgkEPNiQAnE9dnILLiXEm2jJ0+Fl2AbW/XG13h51C2ENGR+1ci1V4UdubcmE8HO7Hq8SYayHnwSew+bdS1aBY1RDsDWComYY9dbNg+bDM72UegshteZ+TwMuEe6KQHHkeBszzPzzuL2Zsl1owOFGuT12uzJmDzVgdIfDvvbGqF1BJwr7/EcUEOBqr8pondQoua15BDs8z8DkKoe64A9vmyUYSCFL74XB1YY+EeTqymwRnV09g/tjZg9YFoFUJPTlQnKBRsNp4JeMFQbBt7h1fVnjBUR/xNSP0JuPTgZ3DsEkdRtxg1I/gVGHNSOxMRE155mXKDmN1hMDeTvoeYVkOSQ3DCRYFCd5JlsH2ngTPM75JRmqNr8+osNhm+Xzc+s9lCimCpYlwNKK7d99TkWXibtp+8gu4/8trEug8zjOQ4Bjf1ZftnxuD7eQAHPRhUNGEI4wWWyg5A6OYNylmHnjqmO25jSwou27RA4H/584RcIqvoDwAqEN+PmlssHM4x0DhPg815glFPfve4LjQ+WoIgXveCFd7qW7AfqsKD0bSRJtOsiiosSqj/2nhU6tfXdhdjCJmy15DEEYHKrf/8zkIO7beC4soBJ6nESs86GZCXmRmZ+xdr4Y50K06fr9KD+6IIaj51ooAWF1Ma9s3sfaWFX5vKvghtqBXwsorErtg0HlcY1C6MdS0ZA9NHs2QXA0hsFslCw1jYggOlF3ltdkKVhPTdtpcmV1W80e2es7mkwPPY/nq4CqQo9+iCDelHysi4E5FOVCPVvgnEgJVwdAnQwhftXUvCCF8OoTw9er7tmp9CCH8Wgjh4arY6JsnucaR6A9g2ErkoHasUofFpz5pPb2C5cK0Zsu0uSJnkWsiB10+kYgXBPW8F6n98iP4QOfmgex9r03oGYjiJihxqRmTagK/Cbw7W/ch4DMxxnuAz1S/IdUcvKf6fIBUeHQ6xF3YUEhlCK1I8rLOKhGkYHVxMOXxZ2FK+gjf5CvwvH/xA5w/kGsG+T4DO4fnCkhguP9k/N0diRjjZ0mF3xz3Ax+vlj8O/Iit/62Y8Dng1qzu4MnwwrvTZKO9HnRaaQpyAvWfIdNgHjhKVV1m23OdsOyDgFRxt+f1nfsBPINQyKc9d7PBTQWvKCStY4M6lXm8yTyNT+AlMcbHAarv26v1Lwe+a/s9Uq07FVp3vQHYgP2DVEPgxkNKSkrlmWWn1B84roFdAC5VnyIMCsYhLz2W1wHUenn0Pc3Z6wfA4aKlWnYh4dRiT2VWuLAZ8wgRNvWKQ70phPABkrlwJIa9/aT+t6gnHB12qdlQkDZO6xxskUgn0izGaRc3V9fetOMOKNTlgsPwztrkA/C8AbXhpoxD5w84pdjJQ33gYvWd9w3VGWzGNJrAE1Lzq+8nq/WPAK+w/e4AHssPnnjege5BVU+xcmy0JdU8n3pau060S4/X5ufcINXFF0NRntpbgcsc5XiZL/JXWDSTxeCorqSO7hqrE4sUEnQHoYf39MkzEHPSkEcbJCRU6HQ+tOFPAg9Uyw8Af2jrf6KKErwduCKz4VTY30/1AwjQk0LhHW5apqC8rDCaeeW4RJKy2n/L9tkmCYGLLKaqUX6v7oEuODu4117I24NGdI3kUvs95Tg3CcQn8IxAf8d6z4E02ouWnNORxyv9E5kDIYRPAD8AvCiE8AjwPwL/EPi9EMJPAX8N/Fi1+6eA9wIPk9ySPznJNcaiP6zqKUZodaC3Qy0Nc2bUaaA/SOpTfj7FXhV+8dBMh6QROEFjl7MxDTwcBKO2okdNfH3B/OFCeZCtH9h2OQHVUfNqQdrP+TA+YEEdFlRGrQqPblEXHBmSTNjx5sBEQiDG+P4xm97VsG8EfnaS806E7kGqHxAibG1Db5dR22kWHU5/vDQBIZBGeqlpmlaqZb/1EjftmFmXPUq53NUAACAASURBVM9DoRskzUNTs3uDEZq80cvuSV9FTPq/+mjucwtI7Xe13jUCSO1SFYPVFqUpyAcgB/mm7eM5COPvcflzB1oVNXjotdy9tNJ4r+fRkHqvkdtLNQmy/9XZNfJLRZPNJaGgeoe7zLaWXf5C5fnVCLJRXTPPppRAOr4hFJwE3qFP4pPydyDHYI/Rkd61AahHeqcKy4Hdso/apgYgJxe5QDmMJRECAdpNpaZb0N6AWEnN3g61t1R8gdNAndttqyaNQqO+ykJt2kd/fpXSutGGnryzMllmRWfWeaSJyBxxzrjuT9OvuUqpUFHB7DGpYM1Df+4YdB6ABLdmHZLTUBmBgoSINFHXZiG1Zw1Q7js4jKUQAmHrMp1XNU5eTG/vCtCFzS3oivkkDcC9pydp5G7DO2EDRh2NyllXvvo2tb1VcbjbnVTgpE2aB2E4BK4DV05wP5NCmodCQM4wk6CSauj2p9uVBQm3cPp3lGf25VAb8s7pHH5vb54xKAdhl5oMp/cqm3/TjtXgpUHBNWURh7Q83mm9FEKgtdHhppe++ND6GCPPfet56PXhQic5BumQbOE2dS32k2oEUvH3GY2pQvqT96rl7eo6Ch9uUQuE6oW1OtCpkkM6GzC8CH3ZZadFHvFQRMIdSNWkrDc0GJkzagh5hy8CoMYGKbQ7D0Et5AlBG9RhQDmY3azQdr33nq3Tdg+Jy0TdtmPU8X0eDpmJSy4ECBE6Dep4jBBasNlJJsFBL/2OkB5edMiTCgE5ZXp2HsH/rC6JGyD1vxIAra1KuAfY6NT32elAbEHfbbvTQJ1dnfkidQPSCH/AqK3YZTxpqmgBh/HoBPvof8sjMYLU+/w96714BMsz+zTy5zRh31fCQl20xaifydmAnjSUT3R6fFtcDiEQI8PhYSEQIyk0SCfNNzCQc1B/1mli4W5rOcfa7W5pB09V17nIDV9A6KQRn2EiLnU6yWk5JJkFUd7aaYSANBNpAJvV93VGqylh33lEwkeZIgBGMakzOY+wNCF/x+7Yc6qvnM+e3y//lkqPVTU0bwiIAbW26u/b1XznHnhWodrhRY6rubEUjJII9PvNH3oxSYMhcEGjtkvWk5gDbdLILtVKTrZtRpMyxAyEVPduN+0X2sk30W4lE0DzIHbaSVsJrfThEpNXznW07FsaiVRE2You7Y/CLKMT5xEnIXZNIjTyCIz+f6cAO0nIpw3TtzsC5R+Q+edRHrJzeQqyruNU9hUIEQ4HQ65fbSh/FCNsRTjoQ9/VoW3qyRV8FD8OGlXl1dfxWySJu0ddhkn2/w7wNPDipAG0WpX5Euo+u9muhG8fBh3Yvgz7WxxV0ukwREWWXadMSahHBDl4ThoWLYShw7iVVJbstCHmHHln9OhNbjbkHVJOQ6cXb5Leubqo2oVGfncKwmgugjRm1RqUIGrGUmgCaaRv+ESqmYaG0ArVvIPO7x9nk429kO3rWVYb1M6/QN155eC7AjxTJzC1q/3kdCcmn0WoJHPrNB1VrC8t+yjiRKScB6Dn2GQ8igA4DKnis0QTQUusPc8m1EdhZDcf3NbXQJVHD7zb6jyuyUqT0LH71acZS6EJEAKEMQk4KrBCqP5jqVKu+kwK9VrZ/PKiyu7X8jXq7EC9vGeBF0BvO2kDkUo4tSCG6n1HGAbYrcKaJ4ZXgnGnn545J6Y4eaiUWDsZnpvDOd3557a6x+ndpyMhAbVN30T9hjpUqDbs7V9teseOkcar849vI8shBIZAd0xnltOt300jcB9qR4e85JNCklEjqiSm/kgPD16j/gP1QvZSqbN22zgY7VTnoN3mxuxIXOH0SpauqQsMSVJ8XEw6HrGtYDFwtd+7WF77wj34MvWcBhxtf7VBzznwKIMTxGQKiN0qR3WzNrAcQgCqUMChlbDZgu4QWoNExhmhWMpRNikG1NwCjbD5NE7XbV/hBSRnn/bfSsIghHRPUcuD6jn2T3hfTZCa7+WjcvhLF6bNqiyYHk02f+4P6Ni3hIVrCQM7bjM7FtsOo7k0HjVbpdyBQGLeNaG/n8hCrVayu4HaaZKz/Y6De1hz1WtIrVW0SebBteq3KglVna7fT+HK2CHNkNxKZkD0mP006rnUOKlwTQIg2CfPVitYDjhF2B2HMOrZV2KQ1uX+LhGHvB3IN+BRgn3qNHftL6Gw7I5BqP0e+Yd2IuAMWylENyLVTsoWdHXMp3o2dR9Inf4mahnZtWP2YbCXztUfpI4/GKYJUnv9pBXMZCTWeW54Hw2uDamxjMNyyPn1hDqf3pHUeHdIu2qvDuuqfR5xgNE2q+P18SpEItO5Q/wwlqKFhHZg43LDrcRI97mNxCZ0B2vayHShL//Dh9Q03G1q9cuJHPozVaWl4gG0K6dgt5/8F70edWGHaSA7ronw495n2ZL5aKOkk/y4oinUEBf/6Gq8p4PaljvzpN1B/a7kQNY71DyLapca0fU+c0HhA6GzCw8YncBkyc2BEAKbG4fJGzFGuiGm+PzONUYzp06SwunYpe7o+qO71GHBmxmNQEAdhtmj5ihULydWL2/QTQLh4FlqSu9J4UItd/z4dmePQa2l+HHbHGaKFQEwik2Sv0elvGcF75hNWqv7cryd6X17STAf8XUezxCVoFBCnRLLpBVIG5mixuCYiUf+lxDCX1aTi/xBCOHWav2dIYS9EMKXqs8/Oe78wrAdGj6tylfQTaZAANqSikfFxY+COoarYBIAYg7KVHApndtelad1CPQGicdwsEMqtThNZ2tly3kc2J2a7ijVC1fmWXEOHg91tKa2dBIzU1AB2jwt3v1QHv5VO3RGIYz6k9SWNLB4VuDQjtFzqD1sM5p6PF7ITeIT+E0OTzzyaeANMcb/APgr4Bds2zdijPdWn5+Z4Pypu/UbPoMI3S6EAbQ2EhknOg3zNMU91Znlda/s+xsZWyIm7NkxXtWV6tr76bjBMDkv6ZOYhaolcBrkNo+PAlsN23QvkbrM2YDk01BDWh63z/JB73NcctBJoAxXxfNzfodnFHr41+37nB0ocyL3K7h5KGapXyevhn00MepYcyDG+NkQwp3Zuj+yn58D/vPjznMk+gf0n/3mmBtow9ZNsH+tStbZhqHs99P4BTwp44BkHihDEGrNQKFCLyrqnvhO2mdQddxujyQEZllfUBmSKoIiLcXposotUMPzmnN6noJmRBJ1eNpZjGDUZpda7xRwXQ9G04H1nvL0YsX63c5v2T7uUHSnorarypXMVxXiab7zafFfAb9rv+8KIfw70r/738cY/9+mg0bmHQjAd77WtBe86Ptgfw82N5MW0PcRLmdiTYp9RtV9sat0MxIAHVJoUIQd1zxU3OG5av2V6rhZ2JZqBLIJnX+u+/QSYxIEagRXKZ1/EgyZHXNQRBy1Ty8L5oJBA4k6rnv/3e53LSX3g0nj2OOwc1t1J3btPO5DOIyphEAI4Zequ/ztatXjwCtjjM+EEP428C9CCK+PMR6aQD7G+FHgo9V5YnOuTYSXbcLOTtICdveqdGLZQ7l39CSQAPAagTAqLS9R//l9Uie/SP1yL5P++OdIQmAWWoBGe0lxL4Oul6nXpoISilwomar4A84GuRaad15su9Y7GxDqwcSPlaCXna82LyEgvovzSDxcqPPrPFscFQE5tcEYQngA+HvAf1lVGCbGeBBjfKZa/gLwDeA1p70GALGXCncM+0kQjJCKRLQ4LlZ+FPQHyoZyaeSlnLS9S+r416k1hl1mF2bysKDCk2I6etRij3qUkGdY/g1YksDPOUeubTUNRnofHqv3kmLq1O4zyB2DMu881Kg5Brt2vJuC0hBVqmz8QHmqlhJCeDfw3wHvjDHu2voXA8/GGAchhLtJMxOPMfYnxDDAVhv2WimPv9WlrqIzK1JOj9SBXJVXXoH+eDki9ZcdkHwAA5LgmIVdCaPOQamYTjrxxrRL7bPw9UfNo1gwP+SagQ9MeUQgLzWm6I5PMiqhL6htaPA7sN/SJHKHsMze8fUTjhUCYyYe+QVS6/t0CAHgc1Uk4B3A/xRCkE76MzHGfDbjk6HdSaG3Vidx83tuWynZQnkEp0Wfw0k/EgKejeWkDUid8Drjqb2nQRNBSCZATv5x1VCOIO1fsgrPDm6WuiDw0R3q0VimrDQ45wFo2dPm5YuS6eeDU54E5+ciO74ZITYm7pwtQgjjb+JtPwx/cxX6e7DTg/0d6A1huE+ShPMsFnkziT68T62Oy9EjoTBLAXAcNOo3CTwPL21x8uSqgsnQxLrUQOS5HNpHwtu9+9rfk32k0iu79SA7rwsHdXwt922fPPsQ6mzCp7/QNPfn0huOl7cvcz08B6EDg4PkG2hvwLDH/Kf7UjZgqJZ9ooizhJOGxo3wYpmF7Bg1toLZoOndO9dfv139lvB2ISEbXrMLQT2yO7HICWDq3NpH79irW8t3IHPD6efNWHomyfXvPARxUGnCw0oYuB0/T3RJKr8EACxmdNULPkrFlwXmqqGPBgXzgY/AMKr6O3XX4YVE8qhATgySQJGfQOO2BIUEvecTbDOqZbi/6DCWXgjwna/CwX7iB3RaSRAMr3J2Nq8888sGTYIiuANTIc/cPiyYPXI73B11rvbDaHqv1l2khmegup9B7xTqDu2FRAbZb6Ue+77j2/DyCwFIabrdHvT2oXdAncV3gZpNt24Q4wxGO77gjaBgPvB25yO4okh6Ly1Gw80K4+VRHfcTuFkn2995It7BXUiow49LNjuM1RACGxsQeoknsHUJWiobLkm7DshVe6n/amyKCHipafcLFMwHTuRS/oBse2cKenq67HSFl53s45GAnO0nG99nJvLteQUqT1gaj6V3DAKwvQ2xC60LMLyeTIKu/lRRes87mjgRR5kpJYV4vnB2HozSzj1Gr04qB54X+djIjoXDFPAWteNQpDZ3EOZJZ9vUnBftezSZbjU0gUs3QWc71RqklXwDN6TpOmkDJ8FRkYSC2cD9ARq5NbcFjGYBOjHI1XRPGlLnllnno7hnEEq4eJUiCaNNkpms9++RhGashBDobFVlvkOEra20PHLnq6HQzA8uBJ1vXjA/tLJldUINShqgPC9A9rw6sTqvhw5zb79G+SFJwPTtXDI7cgewfGYiuilf4fgnWVr0v/pvgAibG5UJrAcXjpZ05wueRir4qCLfQHEIzheeoee5/H1GmZ3O7FM8/xK1sHBfgJiA0h48vKj0cWkRzhT0upm6pghKXrK8GavRc7rPQWvTBN4Ahl5VR3/eOkCNILfz9aJzG7FgPmgqBe+JXR4BUKfW+9GMV+rIKmQjZp+Ei5N+tqhzBXJzQsseLXKzo2/bDmM1hACkyUhbrSqBqp9yCQ6VDV8n5B1d06gVnB3yNieNQPa/hwlhNCdEI7/MgK4t+/Tibjp4+DDPFQm2Xvem+ziaNLY6QuCWbdIsPzGlFrfEoZcjZd2EQA6ljOZ8gYL5QYI4d+TJ1hezVR02H7FlrzsjME8kypON3P/gYWInJ3lJMpkOq04WAvjmQ8kxuNGq+r3+DFd91hlexEI+knUXjGcJJ+uokzo7UL4CpcJ7BSLNjK3zKEyoaII7DqE2NbRexyp5TGaFk8hWmTYsPPUtIFQmUAs6WxDEkV5HAeASP3+NcgwWjWC+8PDewNZ5kpCr6UNbrw7uwsDVd9nyzkJ0LoJMCg2A+vZJdQbU5cbGt4XVEQKQsgcj0NqATkhMwhtOltOWIF9ViDiSF7JwlDDh/OGOWnXqPKXYE7ryOgJQj/rSENx0iHas15Xcsut0bb3uSRECFTydwjE4Zt6BXw4hPGrzC7zXtv1CCOHhEMJDIYQfPu78J8KFTjIHOlTRAXGyj+oI5xFS8ychBPnoUTBbeEd1jcAJPjA6uqujesJPyPZXCvsWtdDIC5J67oGvVxFdRRsG2fGHMYkm8JscnncA4FdtfoFPAYQQXge8D3h9dcz/HkKYnU4a+skh2KrUsBBSxGCC6innC3rxJ8luLEJgvmjK6oNRIo8EhOYJcFtdbVgDmkhfAztOJoI0AA2CXpZc5oYnGuUVqUZxrBCIMX4WmLRE2P3A71QFR78FPAy8bcJjj8dXPwuDSiXa6KRowab+uG1KlKAJnuJaMHvk/Ayn9rog0DppCxrh5SvQfh5OlPffM0J1rYvUPgGvcqXfXoLsaPLYND6BD1bTkH0shHBbte7lwHdtn0eqdYcQQvhACOHzIYTPn+iq7UtpKrKNTRi2qyiBZn1ZLRfH2aCUH58fZIfLXtc6eeb12x2Ibtt7DkAeLoRRf4D8CvnMQvpI7XcSESTzwB2Qh3HaXvMR4NXAvaS5Bj6cXdnRqIfEGD8aY3xLU82zI3Ghn0KF3YP0TadmEU6QNllQMDs0Ff5o2zZ56zUqe+w/d/jBKONPA5v7ApwufMDoLMVOQ9a5dQ8bzJwxGGN8IsY4iDEOgV+nVvkfAV5hu94BPHaaa4zF049V2lSA1gCGEdqSkP44xSwoOA1O0m68M7fs02a0A8tWz/NdtE3CQAQ4TSzqQiXYebyzS+UXWcgTi7rUQmPGmkAI4WX280cBRQ4+CbwvhLAVQriLNO/An57mGmPx1Dfh4hZsdioBGqGvQoxCMQsOo/wnk2MSQeAhOU9oC7ZOavgg29/NA6g7sqYOUxZonkS0aeciu6aEjIhGCje6H6IZp5134AdCCPdWZ/828NMAMcavhRB+D/iL6g5+NsY4e6P0wkXYeRram8BB8g/0VFFH8dN1KDSSw5NJcqyzYzAPITulNke+zguH5Os1eov8k5ce96iArptrBRqtFev34qF5dSFPRVYEwUuMe56BpzCrEpFmzBrF8s870ISXv7lynka4ehWGAbrXSDMCqaCCpmcqWE+oM+RcfBhVp487h9N1m7ZptFYln5Zt94iANIGcTahEoxap7XrykbQIMWOxY+QH2KWmEOchxT4piqBqQ881zjuwmjrio1+E0IHNNmxtw/CA9NC3UlfhLYVGRrFuFGIPu/mo6OuOQ65Z5aQenctLiA8bvmWju7NQRC/d2wVqoeFai+eBOCHJR/zcH+Hn1bOOn315NYUApMjAoJschDf6+zZppmB3zKyrgzBvvOuuFZ1UAORQB1SHVudT+5I67jF7j/trH88FkIDQPJd+X15cRNcW3XhIPcOUzAj3HVyw36IjH/1kq4l2hGEfWkNoa9ot8aU3GQ2ZrDvcIbXOaHr+cf+Jtx2v0OOdUuScvMpTnk+g42DUju8war527XilCHuSkDz8W3ZOb+c+X6XzCo6eMXt1hcB3Pp9Yg0TouRc1z8pa94YPo40Rzqdp4M/n6xxNA8JRx7g9r0/euZ0VmJOE3KvvgkQdfovRQrke9pNJoI4tu9+nJ1Nbd86AhIWzBI+eqWt1hQDATZdT7kCnD2Ej+QloUatIOXdgXeH57f77vCBk3/n6o5BHTXJB4fa4x+q9w7sHf4PROQbcy+9FQ/2dqDS4zn/BjhtSs/4iqUPvU/sUpI1AEga581OVhsZjtXvIg/82fbeGsLkNLfkBNDmnpOw6JReNwyqbRT6qjsMk4b6TIPep5BqABhzZ9lLzvRqwziEV/cDOqRHbawbqnBdJWoLXCJQzcZfRHAGZDFALB2kcF4Grxz7pagsBSM96MIDtrepp8kSOo9lSBauA3Jk3Ts330dTXTwv5mjxM5049X47ZJy/u4hwBCQ33Ieh6uv98olOdz39vMCp84CQm3+oLgcsX4PJl2O5AVPjFSRfn0f49CueNOu0qd86UyxGP2X7a63qRT7+WOqNG4m0OmwD6aJJQnccdiEPqSsJUy/vUmYbb1KQiTXMucpLb/y6MOhw2dZqx+kLgm5+H226DGFLY8IY65DgPnWFS+ItfZRMADkd3xj1PyPaZxXPrnPo/ncefRwOcVeikoCbm38DWQd1Z1en79mnZPhepzRAvFKJz+ZwEipCN5wY4Vl8IAFxowX4/lRwbsaPciVOwetD7PA7zEHZ+ToUFYTRl3X0F7o2XDZ+zFLuMCrY88zA3eVQlyMuGSwMQX0D3Jke4MgwnH/jOhxD42r9O05NFVwclfSdxKhUsJxYd4s01DO/07gvII1F5ko/H+qHu1KIGQ63ae7ER8QagJvzkNQTUvp0gtwH8zcRPeX6GyKe+DbfdkfIJ+vvUXtMecG2ht1YwDRZh0rhG6c4/d8qJraf9N6lrA3qVnya6smoHetgQagek5xeITKRzexLQJZKZIA3j5mq/Kyd62nM0RD4P9FIh0pZitWJiqexywXojjFmGw04/jfbOFJSG6fMNOptPI7Lam0Z1nz4szyPItQmqY+Qr8BE+H7NlgujeTldx+xwJARKNuE3iDAQVYJS39vwoPQWnRc73zzEu18RDdq7WyzZXp/Zzt+xYdeZNau++mzoK8zkdWPtp9qK2nWObmjkoh6KWT47z1TM2qpcYArQ6sCsnitdyO29suYKTIa/OI4zzPyjsrGPdYy9vvucDSFDot9rflq0Pdg61Td/mWYGeNAQ1iSinN6tdPz3BfzCKYzWBMfMO/K7NOfDtEMKXqvV3hhD2bNs/OfEdTYNBL5GG2m3oDROlGDhc871gtTAr56B76puaflPSj9RwqffjQrBOK5Zdv0Vtt0NN6Mk5BPIvyLaHNKo7CahLMiu86Kg0lzzf4GSYRBP4TeB/BX5LK2KM/4WWQwgfZtQT8Y0Y472nuptp8dy34FVvhX5VZWgrwF4gOVP0qCep1V+wHFASzbTwTn6Uw9E99XCYcaqO7lWDFb5TJ73AqObgTEGPLOi+PAdAWoEET85Z0DN41uAWJ4kIOI4VAjHGz4YQ7mzaFkIIwN8H/pNTXX0e2NyE1jW4sAW9bnIU9lTKuZgCq4lZCAAhbwPSEtXxZIs7FwBGR1p36Ll2IC6Ap/f66C+Bsc1oApBXFKLaLlPD/QNuMrSpJzFR6vzpMK1j8PuBJ2KMX7d1d4UQ/l0I4f8JIXz/lOc/Ob7+b2AzQKcDwyFs5mqaOwwLCjzG7iq2KlQ1cQIGjJoMLgi8GIg6qjq3yoW5ZuERB13fKxXlhDdpH+IR6Jqn0wL0D0yD9wOfsN+PA6+MMb4J+G+AfxpCuLnpwFNPPjIJHn0oCYFWpf61vBSTyjmXzMICdaameL22Q12vUuaEOqI78DZt2fMH1O60rBFfvyUo3DzxPAIXAuK+6D7EI5iMHjwOpxYCIYQO8J8Bv6t11fRjz1TLXwC+Abym6fhTTz4yEYaw1U4RgnYHgnt0XZUqjsL1Rk7/VS0KheakbjsN2PP/vSN7FApGuQYyA7wmYI9UG0DrfVByR6SHLb1mQe4vmP5fOA1+EPjLGOMjWhFCeLEmIA0h3E2ad+Cb093iKfHdr6a5CYYRhvrzoP4jcw54wfrAWYBQj+BKO1cbkbPPmX/apuU8vVjrvLbAsOEaPnGoTAD3/KtNOv1d15L5ss9pQoI5JgkRfgL4/4DvDSE8EkL4qWrT+xg1BQDeAXw5hPDnwD8DfibGOOlkpjPGPly6DdqtaoYiqKWoz9Si9QXrAw/N5Vl/OfJO74LCpwhTqW+N7L1snVR6aQHSIDp2/iF1KBBqbUHXdL/F7Nrsas47cBK89N+HZ59JcxeOOHn2qGdpKVhPaNS+QO3Aczvf7Xmp4IpUeM6+x/rlG3BTQWPtBnW9v0hdFVjrPCnoMrBTne9Waqf29eo8+4ybTOQINM47cP5d5DddgJ1t6EoSd0kvZbvaIa/cUnD+MI4XoJFfo7ZDI7P8SDqPr9M589RdTxiS3e7CAOrBSD4ImQAiFg2qZTn/NFjJITm7wet85Q404et/mpyDVBOV3HDiKJNLIcOC84km1Tlv9rLdt6iTcHSMO+lcFZfzcN/2V0QAau0h2jHiCGzYPs438Ilz8loE4hTIBJndNHvnXwgADHuJRBSoMgyh/kNViAFqh0vB+UGT03dcs1foWB3e2YU5hdjj/4IEh0cC1IGl6ntGoAseaQLyG2zab9GJ55P/sh5C4Oq34IUvTLkFGy3oyMGiuKsmbBRJpEQLzg+a1OZxqnSXupqP2/ka9bvUTj0nGbnJ4CO4cwvcaah9NCD5+SSANCeB9pWfQueaHdZDCAA8/gW4sJmIQ5sXUrrxDftOL1O2VvEPLAaLFr4i8vgUYuIL9Em2vzqg0oLhcJtRFMojC6E6/oA6ROhTj12wfR0SPHI2ehbibLA+QgAqYRqSNjCUJFb2lnYo0YLFYZ7CdxIzT6r/Js3hQp1ji1EOgB8Po53VswY94UjRB0UUZJrmk5tK/feCIbM1WddLCDz/MNx8Gdp9iBGC4q+S3GKGFb/A+cMkabbOzHMyEaT2oQ6qkVymglcf8kiA+xJa1GFGqGP/HWqhI8GQpzxLY9D+RROYDo9/GToX0jyGW1vU87fLN6CMrYLzhUneqZxzOZff6bnKFHQtwO18DyPmeSrSOLftfKohKAeiBiEnEDkDcfaRrPUTAgBPfgW2OhUNfAs6l6lfmLjhBesFjcKe/CN45Z68GIiOhdrZnM8L4Px/NwlUJQhqjdQ1EC9YanMXbmzDza+Y6mkd6ykEAIbX67Bhx1U0n9K5YH3g7MDcL5STjXJ1XPa+VHwVGpV/QTb9TdShQbETJXicZqwIhKfA23wD2xvQllNyeqyvELj2GNxyc8o0DFA7ZiR1SwGS5cdpbWN1riYclT/gyGchgrqzavRXRMFtf7LjAnU5cQkFr0WggiOVNtJupaS4554/+hFPgPWmyj36ebj59TDoQnsAA0lu5XPnNeUKzh7HlQKb1zl9Hy/lBYejAhrZBXn885mDFCZ0b//Q9nPzQsftVp9qUBo8m1IKZoj11QSEq9+Ajc2qUrGIH87aWm85uXgcFamZRjg4qecoSAB4Ozjqup54JA1Acw9oUhyvZAR1OLDLqD/gKmninPlqpUUIsA/PPw5xCJ1tUseXWSAUQbA4zEMT84Kd4+AMQBGGxkGJPpqVWEQhGC1Nc5cuygAACWRJREFUrhoCXmQEasaq/AUd4JmJn2ZaFCEAwLNwywXobMHlSxD8pebe3YKzxVmaY651+HXVefXJnXLejcQ58YQi5w14KrK0BZkXXqD07HxSkxQVeUUI4Y9DCA+GEL4WQvi5av0LQgifDiF8vfq+rVofQgi/FkJ4OITw5RDCm+f9EDPBkw/BhQtw8QJ0NkkOGZ/pRdNLFcwfxwncpmY7rZBWbcAm80Pqu2f/+TWlxu9X21QURCSgnDnoiUqerahS5U9O+SwnwySaQB/4BzHG1wJvB342hPA64EPAZ2KM9wCfqX4DvIdUVuwe4APAR2Z+1/PCc1+BMEhZh62tVKz0hilwiXokKJgvJHSFvIM3aQcn8Q/I8eYZpJHR3ID8el4y3NX6/NpO7nGfkrQCTUziGqZSiKerGnxaHNuiY4yPxxi/WC1fAx4EXg7cD3y82u3jwI9Uy/cDvxUTPgfcGkJ42czvfF544i+qkuWS0l7h1SeQKJgdjqNpT+LJPwnUSRWbP+54OQddOMn5p/VQCwZPHHL+gUwFL0Hm6cEnm014VjjRsFZNQvIm4E+Al8QYH4ckKIDbq91eDnzXDnukWrc62NuBrcuVdqh8b5V3Kr6B2UMja76uCZM02ZNqa03Zew51WFWl0uguobBhHzH+WqSyYbvUQsDrFBxU55RQmL5g6Gkx8b8VQrgM/D7w8zHGq0ft2rDu0D8813kHpsYzcO0haG/CpsKG29RqG4w6igqOx3HCc1IH4ETN6wTnyLWQcffpDD/5BqTON1GDjeY7YgL0GeUItICnTnj/s8VELTiEsEESAL8dY/zn1eonpOZX3/JmPAI4sfkO4LH8nPOdd2BG2OnBxgW4cBO1FrDFaDWYYh4cj1n+T5N4zY8TKF4DMN9XnTXvGu7tz9N9RfN1+rCHlf0afTu2A8yO+XdaTBIdCMBvAA/GGH/FNn0SeKBafgD4Q1v/E1WU4O3AFZkNq4dH4OpTKeOw7XXgPDNs9qmd5w/LKiib6MDy5jf5KdSBfVLbLnW4L5+HIM838AIjgUQEmuU8i6fDJCyY+4AfB76iKciBXwT+IfB71TwEfw38WLXtU8B7gYdJBtFPzvSOzxzPw+UXwsE2tPrQ65FMA9WHK87C1cW49+Y5/eOO89x/5Qm42i+nn9qItAf5AJZnduzzP+/ALHHxTthVLFiq4B61OlhwPqBwncf4x0Flxg5sXT5lmaoSK9K0x4LQOO9A8WqdBLvfhlu3UgpyW+mibRLBQ78LVhPuB1AH9vBfExTaUwf3oh+qJ+hp6X0WKADGogiBk+L570D30VSDoKUYr9TDkn68mvCR20lAfdvedIxXFOqSRnmZBHIeN4U/lwvLfXfLjINHqslMutSJI0IpSLIaUPP3NOA8LVhEIRidKVimgo/yXlJc9QACyxABOApFCEyDve/C1q3QvkztBBJ5pGQeLj8moR9r7gFFhC4wajrsU2sOXh1IJcVPPF/gmaMIgWlx8EQqY36joYgvLoJR8ROsHppyFeQg9OpBPpch1O9aEYPliQAchSIEZoHed0j141QeGur4sVJJC2aDs2iyuTagDu9zVEjV92KiykPYYhU0AKEIgZnhSRItwpNIXG0sgmA2WEQoNmbfHfuWCaiJQiKLSgQ6LYoQmDmer6Y422Q0C7H81asLZ4t6eTA5AtX5VzM6VFrmPND9G9i4zGgFY9cEilawOlAHV5TAC4LKAbja0aAiBOaF3pPUjkFlG6rzL3/suMDfjxKH9hidXkzZgqul/ucoLXGueJI6anDA4XRTORLLa1guuOffHbsqIaZoQJ/zQBcvrW/u2KMOG4pirM7vaagFy4M8OpCTiTQr0PlAEQJngmdJmoD45ZrauqQgrybOl9AuQuBMcY2aYqxS047iMFwe5IVOz29XOb9PtrTQrDIXq99ejbaUNF8eeLivqQLR+UERAgvDtep7O1tfcg4Wi/XTxooQWDhEL5Wz6fyOOKuB1ahvM0sUIbA0KDkGy4H1E8LLons+TZpweXHF16fHi5j6/vdnciNTYAbPsFCs+v3DfJ/hVU0rl6LGIEAI4fNLXX78GKz6/cPqP8Oq3z8s5hmKOVBQsOYoQqCgYM2xTELgo4u+gSmx6vcPq/8Mq37/sIBnWBqfQEFBwWKwTJpAQUHBArBwIRBCeHcI4aEQwsMhhA8t+n4mRQjh2yGEr4QQvqSZlUMILwghfDqE8PXq+7ZF36cjhPCxEMKTIYSv2rrGe67mkvy16r18OYTw5sXd+Y17bbr/Xw4hPFq9hy+FEN5r236huv+HQgg/vJi7rhFCeEUI4Y9DCA+GEL4WQvi5av1i30GMcWEfEjvmG8DdpHzbPwdet8h7OsG9fxt4UbbuHwEfqpY/BPzPi77P7P7eAbwZ+Opx90yaT/L/IiU1vB34kyW9/18G/tuGfV9Xtact4K6qnbUXfP8vA95cLd8E/FV1nwt9B4vWBN4GPBxj/GaMsQv8DnD/gu9pGtwPfLxa/jjwIwu8l0OIMX6WlNfsGHfP9wO/FRM+B9yqqegXhTH3Pw73A78TYzyIMX6LNEHu2+Z2cxMgxvh4jPGL1fI14EHg5Sz4HSxaCLwc+K79fqRatwqIwB+FEL4QQvhAte4lsZqGvfq+fWF3NznG3fMqvZsPVuryx8wEW+r7DyHcCbwJ+BMW/A4WLQSaqmqsSrjivhjjm4H3AD8bQnjHom9oxliVd/MR4NXAvcDjwIer9Ut7/yGEy8DvAz8fY7x61K4N62b+DIsWAo8Ar7DfdwCPLeheToQY42PV95PAH5BUzSekrlXfTy7uDifGuHteiXcTY3wixjiIMQ6BX6dW+Zfy/kMIGyQB8Nsxxn9erV7oO1i0EPgz4J4Qwl0hhE3gfcAnF3xPxyKEcCmEcJOWgR8Cvkq69weq3R4A/nAxd3gijLvnTwI/UXmo3w5ckcq6TMhs5B8lvQdI9/++EMJWCOEu4B7gT8/6/hwhhAD8BvBgjPFXbNNi38EivaXmAf0rkvf2lxZ9PxPe890kz/OfA1/TfQMvBD4DfL36fsGi7zW770+QVOYeaZT5qXH3TFJF/7fqvXwFeMuS3v//Ud3fl6tO8zLb/5eq+38IeM8S3P/fIanzXwa+VH3eu+h3UBiDBQVrjkWbAwUFBQtGEQIFBWuOIgQKCtYcRQgUFKw5ihAoKFhzFCFQULDmKEKgoGDNUYRAQcGa4/8H2yinP41o1akAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for X_batch, y in img_gen:\n",
    "  x = X_batch[0]\n",
    "  print(y[0])\n",
    "  print(X_batch.max(), X_batch.min())\n",
    "#   x = x.astype('float32') / 255.0\n",
    "  x = preprocess_input(x, data_format='channels_last')\n",
    "  plt.imshow(x.astype('int'))\n",
    "  plt.show()\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "156/156 [==============================] - 31s 198ms/step - loss: 1.2210 - acc: 0.5038 - val_loss: 1.0667 - val_acc: 0.5000\n",
      "Epoch 2/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1975 - acc: 0.5176 - val_loss: 1.0424 - val_acc: 0.4944\n",
      "Epoch 3/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1823 - acc: 0.5196 - val_loss: 1.0488 - val_acc: 0.5284\n",
      "Epoch 4/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1565 - acc: 0.5373 - val_loss: 1.0468 - val_acc: 0.5164\n",
      "Epoch 5/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1527 - acc: 0.5401 - val_loss: 1.0538 - val_acc: 0.5400\n",
      "Epoch 6/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1381 - acc: 0.5480 - val_loss: 1.0464 - val_acc: 0.5548\n",
      "Epoch 7/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1253 - acc: 0.5575 - val_loss: 0.9938 - val_acc: 0.5592\n",
      "Epoch 8/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.1080 - acc: 0.5650 - val_loss: 0.9734 - val_acc: 0.5816\n",
      "Epoch 9/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0974 - acc: 0.5663 - val_loss: 0.9388 - val_acc: 0.6044\n",
      "Epoch 10/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 1.0743 - acc: 0.5799 - val_loss: 0.9515 - val_acc: 0.6112\n",
      "Epoch 11/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0604 - acc: 0.5921 - val_loss: 0.9258 - val_acc: 0.6300\n",
      "Epoch 12/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 1.0567 - acc: 0.5921 - val_loss: 0.9321 - val_acc: 0.6272\n",
      "Epoch 13/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0421 - acc: 0.6029 - val_loss: 0.9332 - val_acc: 0.6560\n",
      "Epoch 14/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 1.0349 - acc: 0.5990 - val_loss: 0.9213 - val_acc: 0.6592\n",
      "Epoch 15/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0265 - acc: 0.6104 - val_loss: 0.9273 - val_acc: 0.6416\n",
      "Epoch 16/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0130 - acc: 0.6165 - val_loss: 0.8993 - val_acc: 0.6496\n",
      "Epoch 17/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 1.0129 - acc: 0.6206 - val_loss: 0.8982 - val_acc: 0.6560\n",
      "Epoch 18/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9908 - acc: 0.6283 - val_loss: 0.8796 - val_acc: 0.6804\n",
      "Epoch 19/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9822 - acc: 0.6367 - val_loss: 0.8676 - val_acc: 0.6968\n",
      "Epoch 20/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.9694 - acc: 0.6391 - val_loss: 0.8651 - val_acc: 0.6916\n",
      "Epoch 21/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9704 - acc: 0.6415 - val_loss: 0.8382 - val_acc: 0.6996\n",
      "Epoch 22/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9560 - acc: 0.6462 - val_loss: 0.8265 - val_acc: 0.7032\n",
      "Epoch 23/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9522 - acc: 0.6539 - val_loss: 0.8508 - val_acc: 0.6988\n",
      "Epoch 24/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.9454 - acc: 0.6568 - val_loss: 0.8169 - val_acc: 0.7144\n",
      "Epoch 25/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9409 - acc: 0.6614 - val_loss: 0.8113 - val_acc: 0.7168\n",
      "Epoch 26/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.9304 - acc: 0.6655 - val_loss: 0.8158 - val_acc: 0.7232\n",
      "Epoch 27/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9152 - acc: 0.6718 - val_loss: 0.8118 - val_acc: 0.7244\n",
      "Epoch 28/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.9090 - acc: 0.6775 - val_loss: 0.8136 - val_acc: 0.7244\n",
      "Epoch 29/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.8959 - acc: 0.6814 - val_loss: 0.8016 - val_acc: 0.7248\n",
      "Epoch 30/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8977 - acc: 0.6841 - val_loss: 0.8093 - val_acc: 0.7368\n",
      "Epoch 31/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8873 - acc: 0.6862 - val_loss: 0.7744 - val_acc: 0.7400\n",
      "Epoch 32/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8821 - acc: 0.6904 - val_loss: 0.7865 - val_acc: 0.7436\n",
      "Epoch 33/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8757 - acc: 0.6947 - val_loss: 0.8164 - val_acc: 0.7308\n",
      "Epoch 34/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8673 - acc: 0.6987 - val_loss: 0.8032 - val_acc: 0.7624\n",
      "Epoch 35/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8557 - acc: 0.7075 - val_loss: 0.7984 - val_acc: 0.7524\n",
      "Epoch 36/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8476 - acc: 0.7143 - val_loss: 0.7954 - val_acc: 0.7644\n",
      "Epoch 37/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8410 - acc: 0.7166 - val_loss: 0.7543 - val_acc: 0.7708\n",
      "Epoch 38/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.8379 - acc: 0.7202 - val_loss: 0.7682 - val_acc: 0.7668\n",
      "Epoch 39/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8228 - acc: 0.7292 - val_loss: 0.7715 - val_acc: 0.7784\n",
      "Epoch 40/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.8218 - acc: 0.7238 - val_loss: 0.7506 - val_acc: 0.7872\n",
      "Epoch 41/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8165 - acc: 0.7314 - val_loss: 0.7578 - val_acc: 0.7820\n",
      "Epoch 42/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.8091 - acc: 0.7387 - val_loss: 0.7401 - val_acc: 0.7936\n",
      "Epoch 43/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.8095 - acc: 0.7383 - val_loss: 0.7463 - val_acc: 0.7804\n",
      "Epoch 44/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.7981 - acc: 0.7417 - val_loss: 0.7373 - val_acc: 0.7892\n",
      "Epoch 45/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7780 - acc: 0.7540 - val_loss: 0.7455 - val_acc: 0.7732\n",
      "Epoch 46/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7851 - acc: 0.7456 - val_loss: 0.7459 - val_acc: 0.7836\n",
      "Epoch 47/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7739 - acc: 0.7574 - val_loss: 0.7343 - val_acc: 0.7844\n",
      "Epoch 48/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.7660 - acc: 0.7618 - val_loss: 0.7362 - val_acc: 0.7828\n",
      "Epoch 49/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7656 - acc: 0.7596 - val_loss: 0.7633 - val_acc: 0.7816\n",
      "Epoch 50/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7532 - acc: 0.7643 - val_loss: 0.7257 - val_acc: 0.7860\n",
      "Epoch 51/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.7402 - acc: 0.7743 - val_loss: 0.7212 - val_acc: 0.7908\n",
      "Epoch 52/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7376 - acc: 0.7777 - val_loss: 0.7349 - val_acc: 0.7936\n",
      "Epoch 53/150\n",
      "156/156 [==============================] - 25s 161ms/step - loss: 0.7397 - acc: 0.7735 - val_loss: 0.7345 - val_acc: 0.7880\n",
      "Epoch 54/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7168 - acc: 0.7864 - val_loss: 0.6946 - val_acc: 0.7896\n",
      "Epoch 55/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7158 - acc: 0.7866 - val_loss: 0.7315 - val_acc: 0.7964\n",
      "Epoch 56/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.7208 - acc: 0.7903 - val_loss: 0.6861 - val_acc: 0.7960\n",
      "Epoch 57/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.7076 - acc: 0.7910 - val_loss: 0.6623 - val_acc: 0.7940\n",
      "Epoch 58/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.7002 - acc: 0.7934 - val_loss: 0.6722 - val_acc: 0.7928\n",
      "Epoch 59/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6850 - acc: 0.8015 - val_loss: 0.6613 - val_acc: 0.8040\n",
      "Epoch 60/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6779 - acc: 0.8089 - val_loss: 0.6681 - val_acc: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6700 - acc: 0.8121 - val_loss: 0.6448 - val_acc: 0.7992\n",
      "Epoch 62/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6728 - acc: 0.8092 - val_loss: 0.6523 - val_acc: 0.8040\n",
      "Epoch 63/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6559 - acc: 0.8163 - val_loss: 0.6541 - val_acc: 0.7956\n",
      "Epoch 64/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6631 - acc: 0.8103 - val_loss: 0.6868 - val_acc: 0.7896\n",
      "Epoch 65/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6580 - acc: 0.8170 - val_loss: 0.6616 - val_acc: 0.8056\n",
      "Epoch 66/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6481 - acc: 0.8237 - val_loss: 0.7174 - val_acc: 0.7988\n",
      "Epoch 67/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6392 - acc: 0.8215 - val_loss: 0.6734 - val_acc: 0.7916\n",
      "Epoch 68/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6354 - acc: 0.8256 - val_loss: 0.6262 - val_acc: 0.7924\n",
      "Epoch 69/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6304 - acc: 0.8274 - val_loss: 0.6840 - val_acc: 0.7932\n",
      "Epoch 70/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6228 - acc: 0.8303 - val_loss: 0.6471 - val_acc: 0.7988\n",
      "Epoch 71/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6105 - acc: 0.8394 - val_loss: 0.6384 - val_acc: 0.7936\n",
      "Epoch 72/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.6012 - acc: 0.8432 - val_loss: 0.6580 - val_acc: 0.8048\n",
      "Epoch 73/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.6053 - acc: 0.8385 - val_loss: 0.6343 - val_acc: 0.7996\n",
      "Epoch 74/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.5976 - acc: 0.8440 - val_loss: 0.6207 - val_acc: 0.7992\n",
      "Epoch 75/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5856 - acc: 0.8525 - val_loss: 0.6365 - val_acc: 0.7976\n",
      "Epoch 76/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5844 - acc: 0.8471 - val_loss: 0.6469 - val_acc: 0.7984\n",
      "Epoch 77/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.5725 - acc: 0.8553 - val_loss: 0.6537 - val_acc: 0.7960\n",
      "Epoch 78/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5775 - acc: 0.8548 - val_loss: 0.6509 - val_acc: 0.8000\n",
      "Epoch 79/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5524 - acc: 0.8655 - val_loss: 0.6458 - val_acc: 0.7992\n",
      "Epoch 80/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5636 - acc: 0.8559 - val_loss: 0.6779 - val_acc: 0.7984\n",
      "Epoch 81/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.5563 - acc: 0.8577 - val_loss: 0.6366 - val_acc: 0.8012\n",
      "Epoch 82/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5481 - acc: 0.8628 - val_loss: 0.6317 - val_acc: 0.7972\n",
      "Epoch 83/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5286 - acc: 0.8730 - val_loss: 0.6529 - val_acc: 0.8036\n",
      "Epoch 84/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5335 - acc: 0.8709 - val_loss: 0.6335 - val_acc: 0.7900\n",
      "Epoch 85/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5296 - acc: 0.8694 - val_loss: 0.5923 - val_acc: 0.7988\n",
      "Epoch 86/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5207 - acc: 0.8760 - val_loss: 0.6344 - val_acc: 0.7904\n",
      "Epoch 87/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.5094 - acc: 0.8773 - val_loss: 0.6227 - val_acc: 0.8008\n",
      "Epoch 88/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5072 - acc: 0.8805 - val_loss: 0.6056 - val_acc: 0.7968\n",
      "Epoch 89/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.5045 - acc: 0.8840 - val_loss: 0.6392 - val_acc: 0.7952\n",
      "Epoch 90/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.5006 - acc: 0.8851 - val_loss: 0.6192 - val_acc: 0.7968\n",
      "Epoch 91/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4960 - acc: 0.8845 - val_loss: 0.6266 - val_acc: 0.7988\n",
      "Epoch 92/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4807 - acc: 0.8899 - val_loss: 0.6536 - val_acc: 0.8004\n",
      "Epoch 93/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4820 - acc: 0.8920 - val_loss: 0.6962 - val_acc: 0.8000\n",
      "Epoch 94/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4766 - acc: 0.8928 - val_loss: 0.6200 - val_acc: 0.7948\n",
      "Epoch 95/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.4684 - acc: 0.8957 - val_loss: 0.6366 - val_acc: 0.7960\n",
      "Epoch 96/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4602 - acc: 0.8991 - val_loss: 0.6066 - val_acc: 0.7984\n",
      "Epoch 97/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4618 - acc: 0.8999 - val_loss: 0.5583 - val_acc: 0.7988\n",
      "Epoch 98/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.4529 - acc: 0.8994 - val_loss: 0.6391 - val_acc: 0.8028\n",
      "Epoch 99/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4393 - acc: 0.9067 - val_loss: 0.5817 - val_acc: 0.8072\n",
      "Epoch 100/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4381 - acc: 0.9070 - val_loss: 0.5944 - val_acc: 0.8044\n",
      "Epoch 101/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4377 - acc: 0.9082 - val_loss: 0.5942 - val_acc: 0.8012\n",
      "Epoch 102/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.4307 - acc: 0.9114 - val_loss: 0.6034 - val_acc: 0.8032\n",
      "Epoch 103/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4217 - acc: 0.9167 - val_loss: 0.6102 - val_acc: 0.7972\n",
      "Epoch 104/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4205 - acc: 0.9143 - val_loss: 0.5893 - val_acc: 0.8048\n",
      "Epoch 105/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.4145 - acc: 0.9125 - val_loss: 0.6091 - val_acc: 0.8012\n",
      "Epoch 106/150\n",
      "156/156 [==============================] - 25s 160ms/step - loss: 0.4086 - acc: 0.9163 - val_loss: 0.6626 - val_acc: 0.8032\n",
      "Epoch 107/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.4041 - acc: 0.9189 - val_loss: 0.6350 - val_acc: 0.7984\n",
      "Epoch 108/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3970 - acc: 0.9221 - val_loss: 0.6868 - val_acc: 0.7924\n",
      "Epoch 109/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3903 - acc: 0.9204 - val_loss: 0.6336 - val_acc: 0.7908\n",
      "Epoch 110/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3920 - acc: 0.9211 - val_loss: 0.6500 - val_acc: 0.7976\n",
      "Epoch 111/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3836 - acc: 0.9267 - val_loss: 0.6385 - val_acc: 0.7920\n",
      "Epoch 112/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3766 - acc: 0.9312 - val_loss: 0.6854 - val_acc: 0.7892\n",
      "Epoch 113/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3745 - acc: 0.9311 - val_loss: 0.6230 - val_acc: 0.7992\n",
      "Epoch 114/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3775 - acc: 0.9265 - val_loss: 0.6215 - val_acc: 0.7948\n",
      "Epoch 115/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3674 - acc: 0.9301 - val_loss: 0.5643 - val_acc: 0.7928\n",
      "Epoch 116/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3541 - acc: 0.9373 - val_loss: 0.5867 - val_acc: 0.7952\n",
      "Epoch 117/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3684 - acc: 0.9308 - val_loss: 0.6070 - val_acc: 0.7916\n",
      "Epoch 118/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3564 - acc: 0.9358 - val_loss: 0.5692 - val_acc: 0.7968\n",
      "Epoch 119/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3595 - acc: 0.9317 - val_loss: 0.5821 - val_acc: 0.7852\n",
      "Epoch 120/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3446 - acc: 0.9396 - val_loss: 0.6300 - val_acc: 0.7860\n",
      "Epoch 121/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3408 - acc: 0.9388 - val_loss: 0.5837 - val_acc: 0.8008\n",
      "Epoch 122/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3403 - acc: 0.9406 - val_loss: 0.6860 - val_acc: 0.7956\n",
      "Epoch 123/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3394 - acc: 0.9378 - val_loss: 0.6113 - val_acc: 0.7892\n",
      "Epoch 124/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3355 - acc: 0.9417 - val_loss: 0.6559 - val_acc: 0.7896\n",
      "Epoch 125/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3262 - acc: 0.9437 - val_loss: 0.6234 - val_acc: 0.7876\n",
      "Epoch 126/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3234 - acc: 0.9442 - val_loss: 0.6010 - val_acc: 0.7956\n",
      "Epoch 127/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3231 - acc: 0.9428 - val_loss: 0.5860 - val_acc: 0.7992\n",
      "Epoch 128/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3231 - acc: 0.9447 - val_loss: 0.6008 - val_acc: 0.8000\n",
      "Epoch 129/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3064 - acc: 0.9502 - val_loss: 0.5832 - val_acc: 0.8008\n",
      "Epoch 130/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3128 - acc: 0.9472 - val_loss: 0.5586 - val_acc: 0.8032\n",
      "Epoch 131/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3088 - acc: 0.9466 - val_loss: 0.6061 - val_acc: 0.7972\n",
      "Epoch 132/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.3053 - acc: 0.9489 - val_loss: 0.6185 - val_acc: 0.7948\n",
      "Epoch 133/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.3055 - acc: 0.9476 - val_loss: 0.6092 - val_acc: 0.7904\n",
      "Epoch 134/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2930 - acc: 0.9510 - val_loss: 0.6247 - val_acc: 0.7936\n",
      "Epoch 135/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2944 - acc: 0.9499 - val_loss: 0.6920 - val_acc: 0.7996\n",
      "Epoch 136/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2920 - acc: 0.9535 - val_loss: 0.5853 - val_acc: 0.7920\n",
      "Epoch 137/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2857 - acc: 0.9556 - val_loss: 0.5721 - val_acc: 0.7980\n",
      "Epoch 138/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2903 - acc: 0.9503 - val_loss: 0.6836 - val_acc: 0.7880\n",
      "Epoch 139/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2777 - acc: 0.9572 - val_loss: 0.5823 - val_acc: 0.7920\n",
      "Epoch 140/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2746 - acc: 0.9547 - val_loss: 0.5941 - val_acc: 0.7836\n",
      "Epoch 141/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2711 - acc: 0.9576 - val_loss: 0.6326 - val_acc: 0.7968\n",
      "Epoch 142/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2668 - acc: 0.9588 - val_loss: 0.5531 - val_acc: 0.7928\n",
      "Epoch 143/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2732 - acc: 0.9546 - val_loss: 0.5487 - val_acc: 0.8000\n",
      "Epoch 144/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2666 - acc: 0.9581 - val_loss: 0.5754 - val_acc: 0.7972\n",
      "Epoch 145/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2732 - acc: 0.9543 - val_loss: 0.6400 - val_acc: 0.7912\n",
      "Epoch 146/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2607 - acc: 0.9593 - val_loss: 0.5967 - val_acc: 0.7976\n",
      "Epoch 147/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2651 - acc: 0.9564 - val_loss: 0.5646 - val_acc: 0.7860\n",
      "Epoch 148/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2571 - acc: 0.9612 - val_loss: 0.5944 - val_acc: 0.7920\n",
      "Epoch 149/150\n",
      "156/156 [==============================] - 25s 158ms/step - loss: 0.2527 - acc: 0.9621 - val_loss: 0.5749 - val_acc: 0.7940\n",
      "Epoch 150/150\n",
      "156/156 [==============================] - 25s 159ms/step - loss: 0.2497 - acc: 0.9612 - val_loss: 0.5132 - val_acc: 0.7824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26cd91daac8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model50.fit_generator(img_gen, validation_data=(val_x, val_y), epochs=3, steps_per_epoch=len(train_x)//BS, verbose=1)\n",
    "\n",
    "# for layer in model50.layers[:7]:\n",
    "#       layer.trainable = False\n",
    "# for layer in model50.layers[7:]:\n",
    "#   layer.trainable = True\n",
    "# for i, layer in enumerate(model50.layers):\n",
    "#    print(i, layer.name, layer.trainable)\n",
    "    \n",
    "model50.fit_generator(img_gen, validation_data=test_gen, epochs=150, validation_steps= len(test_x)//20,steps_per_epoch=len(train_x)//BS, verbose=1,  callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(loss, accuracy) = model50.evaluate(train_x.astype('float32') / 255.0, train_y, batch_size=64, verbose=1)\n",
    "print( 'loss = {:.4f}, accuracy: {:.4f}%'.format(loss,accuracy*100))\n",
    "\n",
    "(loss, accuracy) = model50.evaluate(test_x, test_y, batch_size=64, verbose=1)\n",
    "print( 'loss = {:.4f}, accuracy: {:.4f}%'.format(loss,accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 3s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.49      0.44       100\n",
      "           1       0.33      0.43      0.38       100\n",
      "           2       0.30      0.25      0.27       100\n",
      "           3       0.52      0.43      0.47       100\n",
      "           4       0.70      0.58      0.63       100\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       500\n",
      "   macro avg       0.45      0.44      0.44       500\n",
      "weighted avg       0.45      0.44      0.44       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model50.predict(test_x, verbose=1, batch_size=64).argmax(axis=1)\n",
    "test_true=test_y.argmax(axis=1) \n",
    "print(classification_report(test_true, test_pred, target_names=[\"0\",\"1\",\"2\",\"3\",\"4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.30      0.34       100\n",
      "           1       0.34      0.54      0.42       100\n",
      "           2       0.26      0.21      0.23       100\n",
      "           3       0.53      0.54      0.54       100\n",
      "           4       0.64      0.54      0.59       100\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       500\n",
      "   macro avg       0.44      0.43      0.42       500\n",
      "weighted avg       0.44      0.43      0.42       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model50.predict(test_x, verbose=1, batch_size=64).argmax(axis=1)\n",
    "test_true=test_y.argmax(axis=1) \n",
    "print(classification_report(test_true, test_pred, target_names=[\"0\",\"1\",\"2\",\"3\",\"4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 5s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.59       320\n",
      "           1       0.48      0.70      0.57       320\n",
      "           2       0.61      0.34      0.44       320\n",
      "           3       0.73      0.51      0.60       320\n",
      "           4       0.84      0.73      0.78       320\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      1600\n",
      "   macro avg       0.63      0.60      0.60      1600\n",
      "weighted avg       0.63      0.60      0.60      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model50.predict(train_x, verbose=1, batch_size=64).argmax(axis=1)\n",
    "test_true=train_y.argmax(axis=1) \n",
    "print(classification_report(test_true, test_pred, target_names=[\"0\",\"1\",\"2\",\"3\",\"4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
